{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Time-series forecasting: Rolling multi-step forecasts with Recurrent Neural Networks\nRecurrent Neural Networks (RNN, LSTM, GRU) are capable of learning long-term dependencies from the input sequence, support additional exogeneous features as input. Looking at the results below, they are really good at time-series forecasting out of the box with minimal feature engineering! However, RNNs are also rather tricky to work with since they take in a 3D input, which may be hard for beginners to work with. Hopefully this notebook serves as sufficient code for you to adapt in your own work, but any queries I will try my best to answer here, or via [GitHub](https://github.com/mingboiz)\n\n\n![img](https://github.com/mingboi95/forecasting/blob/main/img/Summary.jpg?raw=true)\n\n\nFor the sake of simplicity, a very simple train-test split is used, with data down-sampled to daily frequency for ease of interpretation. \nThis notebook serves as a guide for forecasting time-series with Deep Learning methods, and it implements the following time-series forecasting with:\n- Multiple features (multivariate time-series forecasting)\n- Multi-step and multiple features forecasting (multivariate, multi-step time-series forecasting)","metadata":{}},{"cell_type":"markdown","source":"# 1. Loading the Data","metadata":{}},{"cell_type":"markdown","source":"The helper evaluation functions are available [here](https://www.kaggle.com/mingboi/rnn-utils)","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom rnn_utils import mae, mse, rmse, mape, evaluate # helper evaluation functions\nfrom keras import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:45:51.985475Z","iopub.execute_input":"2021-12-24T15:45:51.985816Z","iopub.status.idle":"2021-12-24T15:45:51.991963Z","shell.execute_reply.started":"2021-12-24T15:45:51.985776Z","shell.execute_reply":"2021-12-24T15:45:51.991044Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"FILE_PATH = \"/kaggle/input/electric-power-consumption-data-set/household_power_consumption.txt\"\ndf = pd.read_csv(FILE_PATH, sep=\";\", parse_dates={'ds':['Date', 'Time']}, na_values=['nan', '?'], infer_datetime_format=True,low_memory=False)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:45:54.929632Z","iopub.execute_input":"2021-12-24T15:45:54.929972Z","iopub.status.idle":"2021-12-24T15:46:08.626149Z","shell.execute_reply.started":"2021-12-24T15:45:54.929942Z","shell.execute_reply":"2021-12-24T15:46:08.625276Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Attribute Information\n\n1. ds: Date in format dd/mm/yyyy\n2. time: time in format hh:mm:ss\n3. globalactivepower: household global minute-averaged active power (in kilowatt)\n4. globalreactivepower: household global minute-averaged reactive power (in kilowatt)\n5. voltage: minute-averaged voltage (in volt)\n6. global_intensity: household global minute-averaged current intensity (in ampere)\n7. submetering1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n8. submetering2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n9. submetering3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner.","metadata":{}},{"cell_type":"code","source":"print(f\"Missing values: {df.isnull().sum().any()}\")\n# imputation with the columns means\nfor j in range(0,8):        \n  df.iloc[:,j]=df.iloc[:,j].fillna(df.iloc[:,j].mean())\n# checking for missing values\nprint(f\"Missing values: {df.isnull().sum().any()}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:48:02.529706Z","iopub.execute_input":"2021-12-24T15:48:02.530042Z","iopub.status.idle":"2021-12-24T15:48:02.772131Z","shell.execute_reply.started":"2021-12-24T15:48:02.530015Z","shell.execute_reply":"2021-12-24T15:48:02.771078Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"To simplify the problem, we just wish to forecast the future household electricity consumption, we will do some pre-processing to frame this problem properly:\n- Downsampling from minute-average activate power to daily active power for households\n- Imputation missing values with column mean\n- MinMax Normalization to preserve variable distributions for our Recurrent Neural Networks\n\nThe dataset is now daily France household electricity data from `2006-12-23` until `2010-11-26`","metadata":{}},{"cell_type":"code","source":"df_resample = df.resample('D', on='ds').sum() \ndf_resample.rename(columns={\"Global_active_power\":\"y\"}, inplace=True)\ndf_resample = df_resample[['y']]\ndf_resample.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:48:06.390108Z","iopub.execute_input":"2021-12-24T15:48:06.390462Z","iopub.status.idle":"2021-12-24T15:48:06.474706Z","shell.execute_reply.started":"2021-12-24T15:48:06.390427Z","shell.execute_reply":"2021-12-24T15:48:06.473856Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# 2. Pre-processing","metadata":{}},{"cell_type":"markdown","source":"Here we have some helper functions that help to create some simple lagged features to add into our model. You incorporate more complicated time-series features in your own work.\n\nRecurrent Neural Networks can take in additional features as a 3-D array for input, where the three dimensions of this input are `sample`, `time_steps` and `features`:\n\n1. Samples - One sequence is one sample. A batch is comprised of one or more samples.\n2. Time Steps - One time step is one point of observation in the sample.\n3. Features - One feature is one observation at a time step.\n\nThis means that the input layer expects a 3D array of data when fitting the model and when making predictions, even if specific dimensions of the array contain a single value, e.g. one sample or one feature.","metadata":{}},{"cell_type":"code","source":"def create_lags(df, days=7):\n    # create lagged data for features\n    for i in range(days):\n        df[\"Lag_{lag}\".format(lag=i+1)] = df['y'].shift(i+1)\n    return df\n\ndef create_features(X, time_steps=1, n_features=7):\n    # create 3d dataset for input\n    cols, names = list(), list()\n    for i in range(1, time_steps+1):\n        cols.append(X.shift(-time_steps))\n        names += [name + \"_\" + str(i) for name in X.columns]\n        agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    agg.dropna(inplace=True)\n    agg = agg.values.reshape(agg.shape[0], time_steps, n_features)\n    return agg\n\ndef create_dataset(df, yhat):\n    # yhat needs to be scaled\n    preds = pd.DataFrame(yhat.flatten())\n    temp = pd.concat([df.iloc[:,0], preds])\n    temp.columns = ['y']\n    date_idx = pd.date_range(start='2006-12-23', periods=temp.shape[0])\n    temp.set_index(date_idx, inplace=True)\n    return temp","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:48:13.473127Z","iopub.execute_input":"2021-12-24T15:48:13.473471Z","iopub.status.idle":"2021-12-24T15:48:13.486131Z","shell.execute_reply.started":"2021-12-24T15:48:13.473440Z","shell.execute_reply":"2021-12-24T15:48:13.485195Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We preprocess by normalizing all variables first, taking care to avoid data leakage by using our MinMaxScaler on training data only","metadata":{}},{"cell_type":"code","source":"chosen = df_resample.copy()\nchosen = create_lags(chosen)\nchosen.dropna(inplace=True)\n\n# Fit scaler on training data only to prevent data leakage\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_x = scaler.fit(chosen.iloc[:1096,1:])\nscaler_y = scaler.fit(chosen.iloc[:1096,0].values.reshape(-1,1))\n\nx_scaled = scaler_x.transform(chosen.iloc[:,1:])\ny_scaled = scaler_y.transform(chosen.loc[:,['y']])\n\nscaled = np.hstack((x_scaled, y_scaled))\nscaled = pd.DataFrame(scaled, index=chosen.index, columns=chosen.columns)\nprint(scaled.shape)\nscaled.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:48:15.455125Z","iopub.execute_input":"2021-12-24T15:48:15.455478Z","iopub.status.idle":"2021-12-24T15:48:15.509161Z","shell.execute_reply.started":"2021-12-24T15:48:15.455442Z","shell.execute_reply":"2021-12-24T15:48:15.508324Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Train-val-test split","metadata":{}},{"cell_type":"markdown","source":"We a simple train-test split for illustration purposes, where we predict for values from `2010-06-01` onwards for the test set.\n\nTrain - `2006-12-23` - `2009-12-22`  \nVal - `2009-12-23` - `2010-05-31`  \nTest - `2010-06-01` - `2010-11-26`","metadata":{}},{"cell_type":"code","source":"train = scaled[:1096]\nval = scaled[1096:1256]\ntest = scaled[1256:]\nx_train = train.drop([\"y\"],axis=1)\ny_train = train[\"y\"]\nx_val = val.drop([\"y\"],axis=1)\ny_val = val[\"y\"]\nx_test = test.drop([\"y\"],axis=1)\ny_test = test[\"y\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:48:20.962826Z","iopub.execute_input":"2021-12-24T15:48:20.963140Z","iopub.status.idle":"2021-12-24T15:48:20.972012Z","shell.execute_reply.started":"2021-12-24T15:48:20.963111Z","shell.execute_reply":"2021-12-24T15:48:20.971170Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"x_train_np = create_features(x_train, 7, 7)\nx_val_np = create_features(x_val, 7, 7)\nx_test_np = create_features(x_test, 7, 7)\n#print(x_train_np.shape, x_val_np.shape, x_test_np.shape)\ny_test = y_test[:x_test_np.shape[0]]\ny_train = y_train[:x_train_np.shape[0]]\ny_val = y_val[:x_val_np.shape[0]]\n#print(y_train.shape, y_val.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:48:27.545586Z","iopub.execute_input":"2021-12-24T15:48:27.545915Z","iopub.status.idle":"2021-12-24T15:48:27.574755Z","shell.execute_reply.started":"2021-12-24T15:48:27.545882Z","shell.execute_reply":"2021-12-24T15:48:27.574048Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# 3. Forecasting with Recurrent Neural Networks\nHere's a helper function to help us train our RNNs, LSTMs, GRUs, where we then forecast with them to get the normalized predictions","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM,Dropout,GRU\nimport tensorflow as tf\nimport warnings\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.layers import InputSpec\nfrom tensorflow.keras.layers import RNN\n\ndef _generate_dropout_mask(ones, rate, training=None, count=1):\n    def dropped_inputs():\n        return K.dropout(ones, rate)\n    if count > 1:\n        return [K.in_train_phase(\n            dropped_inputs,\n            ones,\n            training=training) for _ in range(count)]\n    return K.in_train_phase(\n        dropped_inputs,\n        ones,\n        training=training)\n\nclass IndRNNCell(Layer):\n    \"\"\"Independently Recurrent Neural Networks Cell class.\n    Derived from the paper [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831)\n    Ref: [Tensorflow implementation](https://github.com/batzner/indrnn)\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        recurrent_clip_min: Can be one of None, -1 or float.\n            If None, clipping of weights will not take place.\n            If float, exact value will be used as clipping range\n            If -1, will calculate the clip value for `relu` activation\n        recurrent_clip_max: Can be one of None or float.\n            If None, clipping of weights will not take place.\n            If float, exact value will be used as clipping range\n            If -1, will calculate the clip value for `relu` activation\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix, used for the linear transformation of the\n            recurrent state.\n            Can be `None` or an available initializer. Defaults to `None`.\n            If None, defaults to uniform initialization.\n            If None, and recurrent_clip_min/max is not None, then\n            it uses those clip values as for uniform initialization.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, must be 2.\n            Mode 1 will structure its operations as a larger number of\n            smaller dot products and additions, whereas mode 2 will\n            batch them into fewer, larger operations. These modes will\n            have different performance profiles on different hardware and\n            for different applications.\n    \"\"\"\n\n    def __init__(self, units,\n                 recurrent_clip_min=-2,\n                 recurrent_clip_max=2,\n                 activation='relu',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer=None,\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=2,\n                 **kwargs):\n        super(IndRNNCell, self).__init__(**kwargs)\n\n        if implementation != 2:\n            warnings.warn(\n                \"IndRNN only supports implementation 2 for the moment. Defaulting to implementation = 2\")\n            implementation = 2\n\n        if recurrent_clip_min is None or recurrent_clip_max is None:\n            recurrent_clip_min = None\n            recurrent_clip_max = None\n\n        self.units = units\n        self.recurrent_clip_min = recurrent_clip_min\n        self.recurrent_clip_max = recurrent_clip_max\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer) \\\n                                     if recurrent_initializer is not None else None\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.implementation = implementation\n        self.state_size = (self.units,)\n        self._dropout_mask = None\n        self._recurrent_masks = None\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n\n        if self.recurrent_clip_min == -1 or self.recurrent_clip_max == -1:\n            self.recurrent_clip_min = 0.0\n\n            if hasattr(self, 'timesteps') and self.timesteps is not None:\n                self.recurrent_clip_max = pow(2.0, 1. / int(self.timesteps))\n            else:\n                warnings.warn(\"IndRNNCell: Number of timesteps could not be determined. \\n\"\n                              \"Defaulting to max clipping range of 1.0. \\n\"\n                              \"If this model was trained using a specific timestep during training, \"\n                              \"inference may be wrong due to this default setting.\\n\"\n                              \"Please ensure that you use the same number of timesteps during training \"\n                              \"and evaluation\")\n                self.recurrent_clip_max = 1.0\n\n        self.kernel = self.add_weight(shape=(input_dim, self.units),\n                                      name='input_kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n\n        if self.recurrent_initializer is None:\n            if self.recurrent_clip_min is not None and self.recurrent_clip_max is not None:\n                initialization_value = min(self.recurrent_clip_max, 1.0)\n                self.recurrent_initializer = initializers.RandomUniform(-initialization_value,\n                                                                  initialization_value)\n            else:\n                self.recurrent_initializer = initializers.RandomUniform(-1.0, 1.0)\n\n        self.recurrent_kernel = self.add_weight(shape=(self.units,),\n                                                name='recurrent_kernel',\n                                                initializer=self.recurrent_initializer,\n                                                regularizer=self.recurrent_regularizer,\n                                                constraint=self.recurrent_constraint)\n\n        if self.recurrent_clip_min is not None and self.recurrent_clip_max is not None:\n            if abs(self.recurrent_clip_min):\n                abs_recurrent_kernel = K.abs(self.recurrent_kernel)\n                min_recurrent_kernel = K.maximum(abs_recurrent_kernel, abs(self.recurrent_clip_min))\n                self.recurrent_kernel = K.sign(self.recurrent_kernel) * min_recurrent_kernel\n\n            self.recurrent_kernel = K.clip(self.recurrent_kernel,\n                                           self.recurrent_clip_min,\n                                           self.recurrent_clip_max)\n\n        if self.use_bias:\n            bias_initializer = self.bias_initializer\n\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name='bias',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.built = True\n\n    def call(self, inputs, states, training=None):\n        if 0 < self.dropout < 1 and self._dropout_mask is None:\n            self._dropout_mask = _generate_dropout_mask(\n                K.ones_like(inputs),\n                self.dropout,\n                training=training,\n                count=1)\n        if (0 < self.recurrent_dropout < 1 and\n                self._recurrent_masks is None):\n            _recurrent_mask = _generate_dropout_mask(\n                K.ones_like(states[0]),\n                self.recurrent_dropout,\n                training=training,\n                count=1)\n            self._recurrent_masks = _recurrent_mask\n\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_masks = self._recurrent_masks\n\n        h_tm1 = states[0]  # previous state\n\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_masks[0]\n\n        h = K.dot(inputs, self.kernel)\n        h = h + (h_tm1 * self.recurrent_kernel)\n\n        if self.use_bias:\n            h = K.bias_add(h, self.bias)\n\n        h = self.activation(h)\n\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'recurrent_clip_min': self.recurrent_clip_min,\n                  'recurrent_clip_max': self.recurrent_clip_max,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(IndRNNCell, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass IndRNN(RNN):\n    \"\"\"Independently Recurrent Neural Networks Cell class.\n    Derived from the paper [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831)\n    Ref: [Tensorflow implementation](https://github.com/batzner/indrnn)\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        recurrent_clip_min: Can be one of None, -1 or float.\n            If None, clipping of weights will not take place.\n            If float, exact value will be used as clipping range\n            If -1, computes the default clipping range for Relu activations\n        recurrent_clip_max: Can be one of None, -1 or float.\n            If None, clipping of weights will not take place.\n            If float, exact value will be used as clipping range\n            If -1, computes the default clipping range for Relu activations\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n            Mode 1 will structure its operations as a larger number of\n            smaller dot products and additions, whereas mode 2 will\n            batch them into fewer, larger operations. These modes will\n            have different performance profiles on different hardware and\n            for different applications.\n        return_sequences: Boolean. Whether to return the last output.\n            in the output sequence, or the full sequence.\n        return_state: Boolean. Whether to return the last state\n            in addition to the output.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n    # References\n        - [Learning to forget: Continual prediction with NestedLSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n        - [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831)\n    \"\"\"\n\n    def __init__(self, units,\n                 recurrent_clip_min=-1,\n                 recurrent_clip_max=-1,\n                 activation='relu',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer=None,\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=2,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if implementation == 0:\n            warnings.warn('`implementation=0` has been deprecated, '\n                          'and now defaults to `implementation=2`.'\n                          'Please update your layer call.')\n        if K.backend() == 'theano':\n            warnings.warn(\n                'RNN dropout is no longer supported with the Theano backend '\n                'due to technical limitations. '\n                'You can either set `dropout` and `recurrent_dropout` to 0, '\n                'or use the TensorFlow backend.')\n            dropout = 0.\n            recurrent_dropout = 0.\n\n        cell = IndRNNCell(units,\n                          recurrent_clip_min=recurrent_clip_min,\n                          recurrent_clip_max=recurrent_clip_max,\n                          activation=activation,\n                          use_bias=use_bias,\n                          kernel_initializer=kernel_initializer,\n                          recurrent_initializer=recurrent_initializer,\n                          bias_initializer=bias_initializer,\n                          kernel_regularizer=kernel_regularizer,\n                          recurrent_regularizer=recurrent_regularizer,\n                          bias_regularizer=bias_regularizer,\n                          kernel_constraint=kernel_constraint,\n                          recurrent_constraint=recurrent_constraint,\n                          bias_constraint=bias_constraint,\n                          dropout=dropout,\n                          recurrent_dropout=recurrent_dropout,\n                          implementation=implementation)\n        super(IndRNN, self).__init__(cell,\n                                     return_sequences=return_sequences,\n                                     return_state=return_state,\n                                     go_backwards=go_backwards,\n                                     stateful=stateful,\n                                     unroll=unroll,\n                                     **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n    def build(self, input_shape):\n        timesteps = input_shape[1]\n\n        if timesteps is None:\n            warnings.warn(\"Number of timesteps was not provided. If this model is being used for training purposes, \\n\"\n                          \"it is recommended to provide a finite number of timesteps when defining the input shape, \\n\"\n                          \"so as to initialize the weights of the recurrent kernel properly and avoid exploding gradients.\")\n\n        self.cell.timesteps = timesteps\n\n        super(IndRNN, self).build(input_shape)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n        self.cell._dropout_mask = None\n        self.cell._recurrent_masks = None\n        return super(IndRNN, self).call(inputs,\n                                        mask=mask,\n                                        training=training,\n                                        initial_state=initial_state,\n                                        constants=constants)\n\n    @property\n    def units(self):\n        return self.cell.units\n\n    @property\n    def recurrent_clip_min(self):\n        return self.cell.recurrent_clip_min\n\n    @property\n    def recurrent_clip_max(self):\n        return self.cell.recurrent_clip_max\n\n    @property\n    def activation(self):\n        return self.cell.activation\n\n    @property\n    def use_bias(self):\n        return self.cell.use_bias\n\n    @property\n    def kernel_initializer(self):\n        return self.cell.kernel_initializer\n\n    @property\n    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer\n\n    @property\n    def bias_initializer(self):\n        return self.cell.bias_initializer\n\n    @property\n    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer\n\n    @property\n    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer\n\n    @property\n    def bias_regularizer(self):\n        return self.cell.bias_regularizer\n\n    @property\n    def kernel_constraint(self):\n        return self.cell.kernel_constraint\n\n    @property\n    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint\n\n    @property\n    def bias_constraint(self):\n        return self.cell.bias_constraint\n\n    @property\n    def dropout(self):\n        return self.cell.dropout\n\n    @property\n    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout\n\n    @property\n    def implementation(self):\n        return self.cell.implementation\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'recurrent_clip_min': self.recurrent_clip_min,\n                  'recurrent_clip_max': self.recurrent_clip_max,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(IndRNN, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config):\n        if 'implementation' in config and config['implementation'] == 0:\n            config['implementation'] = 2\n        return cls(**config)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:38.947280Z","iopub.execute_input":"2021-12-24T15:11:38.947761Z","iopub.status.idle":"2021-12-24T15:11:39.038173Z","shell.execute_reply.started":"2021-12-24T15:11:38.947719Z","shell.execute_reply":"2021-12-24T15:11:39.037255Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def fit_model(m, units, x_train_np, x_val_np, verbose=False):\n    model = Sequential()\n    model.add(m (units = units, return_sequences = True, input_shape = [x_train_np.shape[1], x_train_np.shape[2]]))\n    #model.add(Dropout(0.2))\n    model.add(m (units = units))\n    #model.add(Dropout(0.2))\n    model.add(Dense(units = 1))\n    # Compile Model\n    model.compile(optimizer = 'adam', loss = 'mean_squared_error'\n                  ,metrics=[tf.keras.metrics.RootMeanSquaredError(),\n                            tf.keras.metrics.MeanAbsoluteError(),\n                            tf.keras.metrics.MeanAbsolutePercentageError()])\n    # Fit Model\n    history = model.fit(x_train_np, y_train, epochs=50, batch_size=512, \n                        validation_data=(x_val_np, y_val), verbose=1, shuffle=False)\n    return history,model","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:12:08.178503Z","iopub.execute_input":"2021-12-24T15:12:08.179140Z","iopub.status.idle":"2021-12-24T15:12:08.189135Z","shell.execute_reply.started":"2021-12-24T15:12:08.179090Z","shell.execute_reply":"2021-12-24T15:12:08.187941Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"LSTM_history,LSTM_model = fit_model(LSTM, 64, x_train_np, x_val_np)\nprint(\"_____\"*20)\nprint(\"_____\"*20)\nprint(\"_____\"*20)\nGRU_history,GRU_model = fit_model(GRU, 64, x_train_np, x_val_np)\nprint(\"_____\"*20)\nprint(\"_____\"*20)\nprint(\"_____\"*20)\nIndrnn_history,indRNN_model = fit_model(IndRNN, 64, x_train_np, x_val_np)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:14:34.643409Z","iopub.execute_input":"2021-12-24T15:14:34.643829Z","iopub.status.idle":"2021-12-24T15:14:53.526919Z","shell.execute_reply.started":"2021-12-24T15:14:34.643754Z","shell.execute_reply":"2021-12-24T15:14:53.526135Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport matplotlib\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 12}\n\nmatplotlib.rc('font', **font)\nplt.rcParams['axes.facecolor'] = 'white'\nhistory=LSTM_history\nModel_=\"LSTM\"\npyplot.title(Model_+' Loss')\npyplot.plot(history.history['loss'],'r*-', label='train')\npyplot.plot(history.history['val_loss'],'b*-', label='test')\npyplot.legend()\npyplot.show()\npyplot.title(Model_+' root_mean_squared_error')\npyplot.plot(history.history['root_mean_squared_error'],'r*-', label='train')\npyplot.plot(history.history['val_root_mean_squared_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()\n\npyplot.title(Model_+' mean_absolute_error')\npyplot.plot(history.history['mean_absolute_error'],'r*-', label='train')\npyplot.plot(history.history['val_mean_absolute_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()\n\npyplot.title(Model_+' mean_absolute_percentage_error')\npyplot.plot(history.history['mean_absolute_percentage_error'],'r*-', label='train')\npyplot.plot(history.history['val_mean_absolute_percentage_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:15:35.723032Z","iopub.execute_input":"2021-12-24T15:15:35.723412Z","iopub.status.idle":"2021-12-24T15:15:36.442329Z","shell.execute_reply.started":"2021-12-24T15:15:35.723361Z","shell.execute_reply":"2021-12-24T15:15:36.441370Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport matplotlib\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 12}\n\nmatplotlib.rc('font', **font)\nplt.rcParams['axes.facecolor'] = 'white'\nhistory=GRU_history\nModel_=\"GRU\"\npyplot.title(Model_+' Loss')\npyplot.plot(history.history['loss'],'r*-', label='train')\npyplot.plot(history.history['val_loss'],'b*-', label='test')\npyplot.legend()\npyplot.show()\npyplot.title(Model_+' root_mean_squared_error')\npyplot.plot(history.history['root_mean_squared_error'],'r*-', label='train')\npyplot.plot(history.history['val_root_mean_squared_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()\n\npyplot.title(Model_+' mean_absolute_error')\npyplot.plot(history.history['mean_absolute_error'],'r*-', label='train')\npyplot.plot(history.history['val_mean_absolute_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()\n\npyplot.title(Model_+' mean_absolute_percentage_error')\npyplot.plot(history.history['mean_absolute_percentage_error'],'r*-', label='train')\npyplot.plot(history.history['val_mean_absolute_percentage_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:15:27.347428Z","iopub.execute_input":"2021-12-24T15:15:27.347868Z","iopub.status.idle":"2021-12-24T15:15:28.059252Z","shell.execute_reply.started":"2021-12-24T15:15:27.347806Z","shell.execute_reply":"2021-12-24T15:15:28.058484Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport matplotlib\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 12}\n\nmatplotlib.rc('font', **font)\nplt.rcParams['axes.facecolor'] = 'white'\nhistory=Indrnn_history\nModel_=\"IndRNN\"\npyplot.title(Model_+' Loss')\npyplot.plot(history.history['loss'],'r*-', label='train')\npyplot.plot(history.history['val_loss'],'b*-', label='test')\npyplot.legend()\npyplot.show()\npyplot.title(Model_+' root_mean_squared_error')\npyplot.plot(history.history['root_mean_squared_error'],'r*-', label='train')\npyplot.plot(history.history['val_root_mean_squared_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()\n\npyplot.title(Model_+' mean_absolute_error')\npyplot.plot(history.history['mean_absolute_error'],'r*-', label='train')\npyplot.plot(history.history['val_mean_absolute_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()\n\npyplot.title(Model_+' mean_absolute_percentage_error')\npyplot.plot(history.history['mean_absolute_percentage_error'],'r*-', label='train')\npyplot.plot(history.history['val_mean_absolute_percentage_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:15:42.011601Z","iopub.execute_input":"2021-12-24T15:15:42.011943Z","iopub.status.idle":"2021-12-24T15:15:42.946645Z","shell.execute_reply.started":"2021-12-24T15:15:42.011914Z","shell.execute_reply":"2021-12-24T15:15:42.945525Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\n#print(x_train_np.shape, x_val_np.shape, x_test_np.shape)\ny_test = y_test[:x_test_np.shape[0]]\ny_train = y_train[:x_train_np.shape[0]]\nX_train = np.reshape(x_train_np, newshape=(x_train_np.shape[0], x_train_np.shape[1]*x_train_np.shape[2]))\nX_test = np.reshape(x_test_np, newshape=(x_test_np.shape[0], x_test_np.shape[1]*x_test_np.shape[2]))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:48:42.945419Z","iopub.execute_input":"2021-12-24T15:48:42.945773Z","iopub.status.idle":"2021-12-24T15:48:42.952783Z","shell.execute_reply.started":"2021-12-24T15:48:42.945742Z","shell.execute_reply":"2021-12-24T15:48:42.951795Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVR, SVR\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\ndef MAPE(Y_actual,Y_Predicted):\n    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n    return mape\nsvm_reg = SVR()\nsvm_reg.fit(X_train, y_train)\n\n# Evaluate on training set\ny_pred = svm_reg.predict(X_train)\nmse = mean_squared_error(y_train, y_pred)\nrmse = np.sqrt(mse)\nprint(\"mse:\",mse)\nprint(\"rmse:\",rmse)\nprint(\"MAPE:\",MAPE(y_train, y_pred))\nprint(\"MAE:\",mean_absolute_error(y_train, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:48:47.538874Z","iopub.execute_input":"2021-12-24T15:48:47.539223Z","iopub.status.idle":"2021-12-24T15:48:47.727071Z","shell.execute_reply.started":"2021-12-24T15:48:47.539191Z","shell.execute_reply":"2021-12-24T15:48:47.726383Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nsvm_reg = RandomForestRegressor(n_estimators=100)\nsvm_reg.fit(X_train, y_train)\n\n# Evaluate on training set\ny_pred = svm_reg.predict(X_train)\nmse = mean_squared_error(y_train, y_pred)\nrmse = np.sqrt(mse)\nprint(\"mse:\",mse)\nprint(\"rmse:\",rmse)\nprint(\"MAPE:\",MAPE(y_train, y_pred))\nprint(\"MAE:\",mean_absolute_error(y_train, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:49:03.201318Z","iopub.execute_input":"2021-12-24T15:49:03.201682Z","iopub.status.idle":"2021-12-24T15:49:05.869515Z","shell.execute_reply.started":"2021-12-24T15:49:03.201653Z","shell.execute_reply":"2021-12-24T15:49:05.868800Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nsvm_reg = GradientBoostingRegressor()\nsvm_reg.fit(X_train, y_train)\n\n# Evaluate on training set\ny_pred = svm_reg.predict(X_train)\nmse = mean_squared_error(y_train, y_pred)\nrmse = np.sqrt(mse)\nprint(\"mse:\",mse)\nprint(\"rmse:\",rmse)\nprint(\"MAPE:\",MAPE(y_train, y_pred))\nprint(\"MAE:\",mean_absolute_error(y_train, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:49:14.872889Z","iopub.execute_input":"2021-12-24T15:49:14.873211Z","iopub.status.idle":"2021-12-24T15:49:16.196474Z","shell.execute_reply.started":"2021-12-24T15:49:14.873180Z","shell.execute_reply":"2021-12-24T15:49:16.195076Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"RNN_preds = RNN_model.predict(x_test_np)\nLSTM_preds = LSTM_model.predict(x_test_np)\nGRU_preds = GRU_model.predict(x_test_np)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.279860Z","iopub.status.idle":"2021-12-24T15:11:54.280610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 RNN","metadata":{}},{"cell_type":"code","source":"resultsDict = {}","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.281912Z","iopub.status.idle":"2021-12-24T15:11:54.282607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rnn_preds = scaler_y.inverse_transform(RNN_preds)\ny_test_actual = scaler_y.inverse_transform(pd.DataFrame(y_test))\nresultsDict['RNN'] = evaluate(y_test_actual, rnn_preds)\nevaluate(y_test_actual, rnn_preds)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.283833Z","iopub.status.idle":"2021-12-24T15:11:54.284723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.plot(rnn_preds, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual, label=\"Actual\")\nplt.title('RNN')\nplt.legend()\nplt.grid(True)\nplt.savefig('1 - RNN.jpg', dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.286110Z","iopub.status.idle":"2021-12-24T15:11:54.287840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 LSTM","metadata":{}},{"cell_type":"code","source":"lstm_preds = scaler_y.inverse_transform(LSTM_preds)\ny_test_actual = scaler_y.inverse_transform(pd.DataFrame(y_test))\nresultsDict['LSTM'] = evaluate(y_test_actual, lstm_preds)\nevaluate(y_test_actual, lstm_preds)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.289111Z","iopub.status.idle":"2021-12-24T15:11:54.289939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.plot(lstm_preds, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual, label=\"Actual\")\nplt.title('LSTM')\nplt.legend()\nplt.grid(True)\nplt.savefig('2 - LSTM.jpg', dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.291267Z","iopub.status.idle":"2021-12-24T15:11:54.292090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 GRU","metadata":{}},{"cell_type":"code","source":"gru_preds = scaler_y.inverse_transform(GRU_preds)\ny_test_actual = scaler_y.inverse_transform(pd.DataFrame(y_test))\nresultsDict['GRU'] = evaluate(y_test_actual, gru_preds)\nevaluate(y_test_actual, gru_preds)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.293352Z","iopub.status.idle":"2021-12-24T15:11:54.294231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.plot(gru_preds, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual, label=\"Actual\")\nplt.title('GRU')\nplt.legend()\nplt.grid(True)\nplt.savefig('3 - GRU.jpg', dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.295689Z","iopub.status.idle":"2021-12-24T15:11:54.296521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Rolling Forecast with RNN, LSTM, GRU\nInstead of forecasting out a very long sequence out `2010-06-01` to `2010-11-23`, 175 days between them is a medium-length sequence. Anecdotally, I can handle up to 300-length sequences with LSTM and GRU, but should experiment with a rolling forecasting schemes to see if it handles the potential vanishing gradient problem. \n\nBy rolling, we mean that we train on initial train set, predict next month. Expand training window to include the predictions from next month, then repeat the following cycle until we have our desired 175 prediction window:\n1. Predict one month ahead\n2. Create features based on predictions\n3. Expand training window to include the predictions\n\nThis means that the maximum output sequence is 30-length long and can be readily handled without vanishing gradient problems.","metadata":{}},{"cell_type":"code","source":"chosen = df_resample.copy()\nchosen = create_lags(chosen)\nchosen.dropna(inplace=True)\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaler_x = scaler.fit(chosen.iloc[:1096,1:])\nscaler_y = scaler.fit(chosen.iloc[:1096,0].values.reshape(-1,1))\n\nx_scaled = scaler_x.transform(chosen.iloc[:,1:])\ny_scaled = scaler_y.transform(chosen.loc[:,['y']])\n\nscaled = np.hstack((x_scaled, y_scaled))\nscaled = pd.DataFrame(scaled, index=chosen.index, columns=chosen.columns)\n\ntrain = scaled[:1078]\nval = scaled[1078:1256]\ntest = scaled[1256:]\n\nx_train = train.drop([\"y\"],axis=1)\ny_train = train[\"y\"]\nx_val = val.drop([\"y\"],axis=1)\ny_val = val[\"y\"]\nx_test = test.drop([\"y\"],axis=1)\ny_test = test[\"y\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.297807Z","iopub.status.idle":"2021-12-24T15:11:54.298656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recreate the dataset, and write some helper functions for preprocessing, forecasting","metadata":{}},{"cell_type":"code","source":"## Helper Function\ni = 0\ndef train_test_split(df, i=0):\n    chosen = create_lags(df)\n    chosen.dropna(inplace=True)\n    x_scaled = scaler_x.transform(chosen.iloc[:,1:])\n    y_scaled = scaler_y.transform(chosen.loc[:,['y']])\n\n    scaled = np.hstack((x_scaled, y_scaled))\n    scaled = pd.DataFrame(scaled, index=chosen.index, columns=chosen.columns)\n\n    train = scaled[:1078+i]\n    val = scaled[1078+i:1256+i]\n    test = scaled[1256+i:]\n    \n    x_train = train.drop([\"y\"],axis=1)\n    y_train = train[\"y\"]\n    x_val = val.drop([\"y\"],axis=1)\n    y_val = val[\"y\"]\n    x_test = test.drop([\"y\"],axis=1)\n    y_test = test[\"y\"]\n\n    n_features = len(x_train.columns)\n    return x_train, x_val, x_test, y_train, y_val, y_test\n\nx_train, x_val, x_test, y_train, y_val, y_test = train_test_split(df_resample, i)\nprint(x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.299930Z","iopub.status.idle":"2021-12-24T15:11:54.300770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use a simple for loop here of train, predict, re-train, predict our forecast","metadata":{}},{"cell_type":"code","source":"TIME_STEPS, N_FEATURES = 7, 7\nrnn, lstm, gru = list(), list(), list()\n\nfor i in range(0, len(x_test), 30):\n    temp = df_resample.copy()\n    x_train, x_val, x_test, y_train, y_val, y_test = train_test_split(temp, i)\n    \n    x_train_np = create_features(x_train, TIME_STEPS, N_FEATURES)\n    x_val_np = create_features(x_val, TIME_STEPS, N_FEATURES)\n    x_test_np = create_features(x_test, TIME_STEPS, N_FEATURES)\n    #print(x_train_np.shape, x_val_np.shape, x_test_np.shape)\n    y_test = y_test[:x_test_np.shape[0]]\n    y_train = y_train[:x_train_np.shape[0]]\n    y_val = y_val[:x_val_np.shape[0]]\n    #print(y_train.shape, y_val.shape, y_test.shape)\n    \n    if y_test.shape[0] != 0:\n        RNN_model = fit_model(SimpleRNN, 64, x_train_np, x_val_np)\n        LSTM_model = fit_model(LSTM, 64, x_train_np, x_val_np)\n        GRU_model = fit_model(GRU, 64, x_train_np, x_val_np)\n\n        RNN_preds = RNN_model.predict(x_test_np)\n        yhat_actual = scaler_y.inverse_transform(RNN_preds)\n        rnn.extend(yhat_actual.flatten()[:30])\n        LSTM_preds = LSTM_model.predict(x_test_np)\n        yhat_actual = scaler_y.inverse_transform(LSTM_preds)\n        lstm.extend(yhat_actual.flatten()[:30])\n        GRU_preds = GRU_model.predict(x_test_np)\n        yhat_actual = scaler_y.inverse_transform(GRU_preds)\n        gru.extend(yhat_actual.flatten()[:30])","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.302065Z","iopub.status.idle":"2021-12-24T15:11:54.302912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because we are using the first 7 inputs as sequence to create features, then dropping the NA values, we have to first do:\n```\ny_test_actual[7:]\n```\nto enforce the lengths of the test values and the predictions to be of equal length","metadata":{}},{"cell_type":"code","source":"resultsDict['RNN Rolling'] = evaluate(y_test_actual[7:], rnn)\nevaluate(y_test_actual[7:], rnn)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.304295Z","iopub.status.idle":"2021-12-24T15:11:54.305265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.plot(rnn, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual[7:], label=\"Actual\")\nplt.legend()\nplt.title('Rolling RNN')\nplt.grid(True)\nplt.savefig('4 - RNN (Rolling).jpg', dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.306588Z","iopub.status.idle":"2021-12-24T15:11:54.307375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultsDict['LSTM Rolling'] = evaluate(y_test_actual[7:], lstm)\nevaluate(y_test_actual[7:], lstm)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.308695Z","iopub.status.idle":"2021-12-24T15:11:54.309527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.plot(lstm, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual[7:], label=\"Actual\")\nplt.legend()\nplt.title('Rolling LSTM')\nplt.grid(True)\nplt.savefig('5 - LSTM (Rolling).jpg', dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.310817Z","iopub.status.idle":"2021-12-24T15:11:54.311666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resultsDict['GRU Rolling'] = evaluate(y_test_actual[7:], gru)\nevaluate(y_test_actual[7:], gru)","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.312960Z","iopub.status.idle":"2021-12-24T15:11:54.313814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18,8))\nplt.plot(gru, \"r-\", label=\"Predicted\")\nplt.plot(y_test_actual[7:], label=\"Actual\")\nplt.legend()\nplt.title('Rolling GRU')\nplt.grid(True)\nplt.savefig('6 - GRU (Rolling).jpg', dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.315342Z","iopub.status.idle":"2021-12-24T15:11:54.316176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Results","metadata":{}},{"cell_type":"code","source":"resultsDict","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.317456Z","iopub.status.idle":"2021-12-24T15:11:54.318335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,a =  plt.subplots(3,2, figsize=(25,9))\n\na[0][0].plot(rnn_preds, \"r-\", label=\"Predicted\")\na[0][0].plot(y_test_actual, label=\"Actual\")\na[0][0].legend()\na[0][0].grid(True)\na[0][0].set_title('RNN')\na[0][1].plot(rnn, \"r-\", label=\"Predicted\")\na[0][1].plot(y_test_actual[7:], label=\"Actual\")\na[0][1].legend()\na[0][1].grid(True)\na[0][1].set_title('Rolling RNN')\na[1][0].plot(lstm_preds, \"r-\", label=\"Predicted\")\na[1][0].plot(y_test_actual, label=\"Actual\")\na[1][0].legend()\na[1][0].grid(True)\na[1][0].set_title('LSTM')\na[1][1].plot(lstm, \"r-\", label=\"Predicted\")\na[1][1].plot(y_test_actual[7:], label=\"Actual\")\na[1][1].legend()\na[1][1].grid(True)\na[1][1].set_title('Rolling LSTM')\na[2][0].plot(gru_preds, \"r-\", label=\"Predicted\")\na[2][0].plot(y_test_actual, label=\"Actual\")\na[2][0].legend()\na[2][0].grid(True)\na[2][0].set_title('GRU')\na[2][1].plot(gru, \"r-\", label=\"Predicted\")\na[2][1].plot(y_test_actual[7:], label=\"Actual\")\na[2][1].legend()\na[2][1].grid(True)\na[2][1].set_title('Rolling GRU')\nplt.savefig('Summary.jpg', dpi=200)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-24T15:11:54.319722Z","iopub.status.idle":"2021-12-24T15:11:54.320649Z"},"trusted":true},"execution_count":null,"outputs":[]}]}