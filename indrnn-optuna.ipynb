{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-27T16:05:58.658448Z","iopub.execute_input":"2021-12-27T16:05:58.659104Z","iopub.status.idle":"2021-12-27T16:05:58.712300Z","shell.execute_reply.started":"2021-12-27T16:05:58.658998Z","shell.execute_reply":"2021-12-27T16:05:58.711620Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:05:58.714099Z","iopub.execute_input":"2021-12-27T16:05:58.714580Z","iopub.status.idle":"2021-12-27T16:05:59.591700Z","shell.execute_reply.started":"2021-12-27T16:05:58.714525Z","shell.execute_reply":"2021-12-27T16:05:59.590973Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/hourly-energy-consumption/AEP_hourly.csv\")\nprint(\"=\"*50)\nprint(\"First Five Rows \",\"\\n\")\nprint(df.head(2),\"\\n\")\n\nprint(\"=\"*50)\nprint(\"Information About Dataset\",\"\\n\")\nprint(df.info(),\"\\n\")\n\nprint(\"=\"*50)\nprint(\"Describe the Dataset \",\"\\n\")\nprint(df.describe(),\"\\n\")\n\nprint(\"=\"*50)\nprint(\"Null Values t \",\"\\n\")\nprint(df.isnull().sum(),\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:05:59.595086Z","iopub.execute_input":"2021-12-27T16:05:59.595294Z","iopub.status.idle":"2021-12-27T16:05:59.769327Z","shell.execute_reply.started":"2021-12-27T16:05:59.595268Z","shell.execute_reply":"2021-12-27T16:05:59.768579Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Extract all Data Like Year MOnth Day Time etc\ndataset = df\ndataset[\"Month\"] = pd.to_datetime(df[\"Datetime\"]).dt.month\ndataset[\"Year\"] = pd.to_datetime(df[\"Datetime\"]).dt.year\ndataset[\"Date\"] = pd.to_datetime(df[\"Datetime\"]).dt.date\n#dataset[\"Time\"] = pd.to_datetime(df[\"Datetime\"], errors='coerce').dt.time\ndataset['Time'] = pd.to_datetime(df['Datetime']).dt.strftime('%H:%M:%S')\ndataset[\"Week\"] = pd.to_datetime(df[\"Datetime\"]).dt.week\ndataset[\"Day\"] = pd.to_datetime(df[\"Datetime\"]).dt.day_name()\ndataset = df.set_index(\"Datetime\")\ndataset.index = pd.to_datetime(dataset.index)\ndataset.head(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:05:59.771494Z","iopub.execute_input":"2021-12-27T16:05:59.771764Z","iopub.status.idle":"2021-12-27T16:06:01.221051Z","shell.execute_reply.started":"2021-12-27T16:05:59.771729Z","shell.execute_reply":"2021-12-27T16:06:01.220398Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(df.Year.unique())","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:01.223415Z","iopub.execute_input":"2021-12-27T16:06:01.223884Z","iopub.status.idle":"2021-12-27T16:06:01.230847Z","shell.execute_reply.started":"2021-12-27T16:06:01.223847Z","shell.execute_reply":"2021-12-27T16:06:01.230045Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.head(2)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:01.233342Z","iopub.execute_input":"2021-12-27T16:06:01.233914Z","iopub.status.idle":"2021-12-27T16:06:01.247435Z","shell.execute_reply.started":"2021-12-27T16:06:01.233882Z","shell.execute_reply":"2021-12-27T16:06:01.246502Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nax1 = plt.subplot2grid((1,1), (0,0))\n\nplt.style.use('ggplot')\n\nsns.lineplot(x=\"Year\", y=\"AEP_MW\", data=df)\nsns.set(rc={'figure.figsize':(15,6)})\n\nplt.title(\"Energy consumptionnin Year 2004\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Energy in MW\")\nplt.grid(True)\nplt.legend()\n\nfor label in ax1.xaxis.get_ticklabels():\n    label.set_rotation(90)\n\n\nplt.title(\"Energy Consumption According to Year\")","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:01.249428Z","iopub.execute_input":"2021-12-27T16:06:01.249919Z","iopub.status.idle":"2021-12-27T16:06:02.536027Z","shell.execute_reply.started":"2021-12-27T16:06:01.249881Z","shell.execute_reply":"2021-12-27T16:06:02.535376Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\n\nax1= fig.add_subplot(311)\nax2= fig.add_subplot(312)\nax3= fig.add_subplot(313)\n\n\nplt.style.use('ggplot')\n\ny_2004 = dataset[\"2004\"][\"AEP_MW\"].to_list()\nx_2004 = dataset[\"2004\"][\"Date\"].to_list()\nax1.plot(x_2004,y_2004, color=\"green\", linewidth=1.7)\n\n\ny_2005 = dataset[\"2005\"][\"AEP_MW\"].to_list()\nx_2005 = dataset[\"2005\"][\"Date\"].to_list()\nax2.plot(x_2005, y_2005, color=\"green\", linewidth=1)\n\n\ny_2006 = dataset[\"2006\"][\"AEP_MW\"].to_list()\nx_2006 = dataset[\"2006\"][\"Date\"].to_list()\nax3.plot(x_2006, y_2006, color=\"green\", linewidth=1)\n\n\nplt.rcParams[\"figure.figsize\"] = (18,8)\nplt.title(\"Energy consumptionnin\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Energy in MW\")\nplt.grid(True, alpha=1)\nplt.legend()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:02.537489Z","iopub.execute_input":"2021-12-27T16:06:02.537965Z","iopub.status.idle":"2021-12-27T16:06:03.675786Z","shell.execute_reply.started":"2021-12-27T16:06:02.537928Z","shell.execute_reply":"2021-12-27T16:06:03.675023Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"sns.distplot(dataset[\"AEP_MW\"])\nplt.title(\"Ennergy Distribution\")","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:03.676823Z","iopub.execute_input":"2021-12-27T16:06:03.677052Z","iopub.status.idle":"2021-12-27T16:06:04.721081Z","shell.execute_reply.started":"2021-12-27T16:06:03.677020Z","shell.execute_reply":"2021-12-27T16:06:04.720418Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:04.722329Z","iopub.execute_input":"2021-12-27T16:06:04.722926Z","iopub.status.idle":"2021-12-27T16:06:04.783130Z","shell.execute_reply.started":"2021-12-27T16:06:04.722887Z","shell.execute_reply":"2021-12-27T16:06:04.782321Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"fig = plt.figure()\nax1 = plt.subplot2grid((1,1), (0,0))\n\nplt.style.use('ggplot')\nsns.lineplot(x= df['Time'] , y= df['AEP_MW'], data=df)\nplt.title(\"Energy Consumption vs Time \")\nplt.xlabel(\"Time\")\nplt.grid(True, alpha=1)\nplt.legend()\n\nfor label in ax1.xaxis.get_ticklabels():\n    label.set_rotation(90)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:04.784298Z","iopub.execute_input":"2021-12-27T16:06:04.784699Z","iopub.status.idle":"2021-12-27T16:06:06.600111Z","shell.execute_reply.started":"2021-12-27T16:06:04.784663Z","shell.execute_reply":"2021-12-27T16:06:06.599424Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"NewDataSet = dataset.resample('W').mean()\nNewDataSet=NewDataSet.dropna()\nNewDataSet.plot(figsize=(15,5))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:06.601601Z","iopub.execute_input":"2021-12-27T16:06:06.602056Z","iopub.status.idle":"2021-12-27T16:06:07.063113Z","shell.execute_reply.started":"2021-12-27T16:06:06.602019Z","shell.execute_reply":"2021-12-27T16:06:07.062419Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(\"Old Dataset \",dataset.shape )\nprint(\"New  Dataset \",NewDataSet.shape )","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:07.064278Z","iopub.execute_input":"2021-12-27T16:06:07.064619Z","iopub.status.idle":"2021-12-27T16:06:07.070059Z","shell.execute_reply.started":"2021-12-27T16:06:07.064583Z","shell.execute_reply":"2021-12-27T16:06:07.069262Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"NewDataSet=NewDataSet.fillna(0)\nTestData = NewDataSet.tail(100)\n\nTraining_Set = NewDataSet.iloc[:,0:1]\n\nTraining_Set = Training_Set[:-60]","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:07.074632Z","iopub.execute_input":"2021-12-27T16:06:07.075024Z","iopub.status.idle":"2021-12-27T16:06:07.083378Z","shell.execute_reply.started":"2021-12-27T16:06:07.074958Z","shell.execute_reply":"2021-12-27T16:06:07.082676Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(\"Training Set Shape \", Training_Set.shape)\nprint(\"Test Set Shape \", TestData.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:07.086089Z","iopub.execute_input":"2021-12-27T16:06:07.086553Z","iopub.status.idle":"2021-12-27T16:06:07.092873Z","shell.execute_reply.started":"2021-12-27T16:06:07.086526Z","shell.execute_reply":"2021-12-27T16:06:07.092161Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import sklearn\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:07.093903Z","iopub.execute_input":"2021-12-27T16:06:07.094398Z","iopub.status.idle":"2021-12-27T16:06:07.202303Z","shell.execute_reply.started":"2021-12-27T16:06:07.094363Z","shell.execute_reply":"2021-12-27T16:06:07.201607Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"Training_Set.values","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:07.203372Z","iopub.execute_input":"2021-12-27T16:06:07.203634Z","iopub.status.idle":"2021-12-27T16:06:07.225639Z","shell.execute_reply.started":"2021-12-27T16:06:07.203600Z","shell.execute_reply":"2021-12-27T16:06:07.225000Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Training_Set = Training_Set.values\nsc = MinMaxScaler(feature_range=(0, 1))\nTrain = sc.fit_transform(Training_Set)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:07.226732Z","iopub.execute_input":"2021-12-27T16:06:07.227031Z","iopub.status.idle":"2021-12-27T16:06:07.235510Z","shell.execute_reply.started":"2021-12-27T16:06:07.226996Z","shell.execute_reply":"2021-12-27T16:06:07.234187Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"X_Train = []\nY_Train = []\n\n# Range should be fromm 60 Values to END \nfor i in range(60, Train.shape[0]):\n    \n    # X_Train 0-59 \n    X_Train.append(Train[i-60:i])\n    \n    # Y Would be 60 th Value based on past 60 Values \n    Y_Train.append(Train[i])\n\n# Convert into Numpy Array\nX_Train = np.array(X_Train)\nY_Train = np.array(Y_Train)\n\nprint(X_Train.shape)\nprint(Y_Train.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:07.236746Z","iopub.execute_input":"2021-12-27T16:06:07.236986Z","iopub.status.idle":"2021-12-27T16:06:07.248352Z","shell.execute_reply.started":"2021-12-27T16:06:07.236958Z","shell.execute_reply":"2021-12-27T16:06:07.247442Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Shape should be Number of [Datapoints , Steps , 1 )\n# we convert into 3-d Vector or #rd Dimesnsion\nX_Train = np.reshape(X_Train, newshape=(X_Train.shape[0], X_Train.shape[1], 1))\nX_Train.shape\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM,Dropout,GRU\nimport tensorflow as tf\nimport warnings\nimport tensorflow as tf\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras.layers import InputSpec\nfrom tensorflow.keras.layers import RNN\n\ndef _generate_dropout_mask(ones, rate, training=None, count=1):\n    def dropped_inputs():\n        return K.dropout(ones, rate)\n    if count > 1:\n        return [K.in_train_phase(\n            dropped_inputs,\n            ones,\n            training=training) for _ in range(count)]\n    return K.in_train_phase(\n        dropped_inputs,\n        ones,\n        training=training)\n\nclass IndRNNCell(Layer):\n    \"\"\"Independently Recurrent Neural Networks Cell class.\n    Derived from the paper [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831)\n    Ref: [Tensorflow implementation](https://github.com/batzner/indrnn)\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        recurrent_clip_min: Can be one of None, -1 or float.\n            If None, clipping of weights will not take place.\n            If float, exact value will be used as clipping range\n            If -1, will calculate the clip value for `relu` activation\n        recurrent_clip_max: Can be one of None or float.\n            If None, clipping of weights will not take place.\n            If float, exact value will be used as clipping range\n            If -1, will calculate the clip value for `relu` activation\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix, used for the linear transformation of the\n            recurrent state.\n            Can be `None` or an available initializer. Defaults to `None`.\n            If None, defaults to uniform initialization.\n            If None, and recurrent_clip_min/max is not None, then\n            it uses those clip values as for uniform initialization.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, must be 2.\n            Mode 1 will structure its operations as a larger number of\n            smaller dot products and additions, whereas mode 2 will\n            batch them into fewer, larger operations. These modes will\n            have different performance profiles on different hardware and\n            for different applications.\n    \"\"\"\n\n    def __init__(self, units,\n                 recurrent_clip_min=-2,\n                 recurrent_clip_max=2,\n                 activation='relu',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer=None,\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=2,\n                 **kwargs):\n        super(IndRNNCell, self).__init__(**kwargs)\n\n        if implementation != 2:\n            warnings.warn(\n                \"IndRNN only supports implementation 2 for the moment. Defaulting to implementation = 2\")\n            implementation = 2\n\n        if recurrent_clip_min is None or recurrent_clip_max is None:\n            recurrent_clip_min = None\n            recurrent_clip_max = None\n\n        self.units = units\n        self.recurrent_clip_min = recurrent_clip_min\n        self.recurrent_clip_max = recurrent_clip_max\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer) \\\n                                     if recurrent_initializer is not None else None\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(recurrent_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        self.dropout = min(1., max(0., dropout))\n        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n        self.implementation = implementation\n        self.state_size = (self.units,)\n        self._dropout_mask = None\n        self._recurrent_masks = None\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n\n        if self.recurrent_clip_min == -1 or self.recurrent_clip_max == -1:\n            self.recurrent_clip_min = 0.0\n\n            if hasattr(self, 'timesteps') and self.timesteps is not None:\n                self.recurrent_clip_max = pow(2.0, 1. / int(self.timesteps))\n            else:\n                warnings.warn(\"IndRNNCell: Number of timesteps could not be determined. \\n\"\n                              \"Defaulting to max clipping range of 1.0. \\n\"\n                              \"If this model was trained using a specific timestep during training, \"\n                              \"inference may be wrong due to this default setting.\\n\"\n                              \"Please ensure that you use the same number of timesteps during training \"\n                              \"and evaluation\")\n                self.recurrent_clip_max = 1.0\n\n        self.kernel = self.add_weight(shape=(input_dim, self.units),\n                                      name='input_kernel',\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n\n        if self.recurrent_initializer is None:\n            if self.recurrent_clip_min is not None and self.recurrent_clip_max is not None:\n                initialization_value = min(self.recurrent_clip_max, 1.0)\n                self.recurrent_initializer = initializers.RandomUniform(-initialization_value,\n                                                                  initialization_value)\n            else:\n                self.recurrent_initializer = initializers.RandomUniform(-1.0, 1.0)\n\n        self.recurrent_kernel = self.add_weight(shape=(self.units,),\n                                                name='recurrent_kernel',\n                                                initializer=self.recurrent_initializer,\n                                                regularizer=self.recurrent_regularizer,\n                                                constraint=self.recurrent_constraint)\n\n        if self.recurrent_clip_min is not None and self.recurrent_clip_max is not None:\n            if abs(self.recurrent_clip_min):\n                abs_recurrent_kernel = K.abs(self.recurrent_kernel)\n                min_recurrent_kernel = K.maximum(abs_recurrent_kernel, abs(self.recurrent_clip_min))\n                self.recurrent_kernel = K.sign(self.recurrent_kernel) * min_recurrent_kernel\n\n            self.recurrent_kernel = K.clip(self.recurrent_kernel,\n                                           self.recurrent_clip_min,\n                                           self.recurrent_clip_max)\n\n        if self.use_bias:\n            bias_initializer = self.bias_initializer\n\n            self.bias = self.add_weight(shape=(self.units,),\n                                        name='bias',\n                                        initializer=bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n\n        self.built = True\n\n    def call(self, inputs, states, training=None):\n        if 0 < self.dropout < 1 and self._dropout_mask is None:\n            self._dropout_mask = _generate_dropout_mask(\n                K.ones_like(inputs),\n                self.dropout,\n                training=training,\n                count=1)\n        if (0 < self.recurrent_dropout < 1 and\n                self._recurrent_masks is None):\n            _recurrent_mask = _generate_dropout_mask(\n                K.ones_like(states[0]),\n                self.recurrent_dropout,\n                training=training,\n                count=1)\n            self._recurrent_masks = _recurrent_mask\n\n        # dropout matrices for input units\n        dp_mask = self._dropout_mask\n        # dropout matrices for recurrent units\n        rec_dp_masks = self._recurrent_masks\n\n        h_tm1 = states[0]  # previous state\n\n        if 0. < self.dropout < 1.:\n            inputs *= dp_mask[0]\n\n        if 0. < self.recurrent_dropout < 1.:\n            h_tm1 *= rec_dp_masks[0]\n\n        h = K.dot(inputs, self.kernel)\n        h = h + (h_tm1 * self.recurrent_kernel)\n\n        if self.use_bias:\n            h = K.bias_add(h, self.bias)\n\n        h = self.activation(h)\n\n        if 0 < self.dropout + self.recurrent_dropout:\n            if training is None:\n                h._uses_learning_phase = True\n        return h, [h]\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'recurrent_clip_min': self.recurrent_clip_min,\n                  'recurrent_clip_max': self.recurrent_clip_max,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(IndRNNCell, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass IndRNN(RNN):\n    \"\"\"Independently Recurrent Neural Networks Cell class.\n    Derived from the paper [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831)\n    Ref: [Tensorflow implementation](https://github.com/batzner/indrnn)\n    # Arguments\n        units: Positive integer, dimensionality of the output space.\n        recurrent_clip_min: Can be one of None, -1 or float.\n            If None, clipping of weights will not take place.\n            If float, exact value will be used as clipping range\n            If -1, computes the default clipping range for Relu activations\n        recurrent_clip_max: Can be one of None, -1 or float.\n            If None, clipping of weights will not take place.\n            If float, exact value will be used as clipping range\n            If -1, computes the default clipping range for Relu activations\n        activation: Activation function to use\n            (see [activations](../activations.md)).\n            If you pass None, no activation is applied\n            (ie. \"linear\" activation: `a(x) = x`).\n        use_bias: Boolean, whether the layer uses a bias vector.\n        kernel_initializer: Initializer for the `kernel` weights matrix,\n            used for the linear transformation of the inputs.\n            (see [initializers](../initializers.md)).\n        recurrent_initializer: Initializer for the `recurrent_kernel`\n            weights matrix,\n            used for the linear transformation of the recurrent state.\n            (see [initializers](../initializers.md)).\n        bias_initializer: Initializer for the bias vector\n            (see [initializers](../initializers.md)).\n        unit_forget_bias: Boolean.\n            If True, add 1 to the bias of the forget gate at initialization.\n            Setting it to true will also force `bias_initializer=\"zeros\"`.\n            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n        kernel_regularizer: Regularizer function applied to\n            the `kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        recurrent_regularizer: Regularizer function applied to\n            the `recurrent_kernel` weights matrix\n            (see [regularizer](../regularizers.md)).\n        bias_regularizer: Regularizer function applied to the bias vector\n            (see [regularizer](../regularizers.md)).\n        activity_regularizer: Regularizer function applied to\n            the output of the layer (its \"activation\").\n            (see [regularizer](../regularizers.md)).\n        kernel_constraint: Constraint function applied to\n            the `kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        recurrent_constraint: Constraint function applied to\n            the `recurrent_kernel` weights matrix\n            (see [constraints](../constraints.md)).\n        bias_constraint: Constraint function applied to the bias vector\n            (see [constraints](../constraints.md)).\n        dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the inputs.\n        recurrent_dropout: Float between 0 and 1.\n            Fraction of the units to drop for\n            the linear transformation of the recurrent state.\n        implementation: Implementation mode, either 1 or 2.\n            Mode 1 will structure its operations as a larger number of\n            smaller dot products and additions, whereas mode 2 will\n            batch them into fewer, larger operations. These modes will\n            have different performance profiles on different hardware and\n            for different applications.\n        return_sequences: Boolean. Whether to return the last output.\n            in the output sequence, or the full sequence.\n        return_state: Boolean. Whether to return the last state\n            in addition to the output.\n        go_backwards: Boolean (default False).\n            If True, process the input sequence backwards and return the\n            reversed sequence.\n        stateful: Boolean (default False). If True, the last state\n            for each sample at index i in a batch will be used as initial\n            state for the sample of index i in the following batch.\n        unroll: Boolean (default False).\n            If True, the network will be unrolled,\n            else a symbolic loop will be used.\n            Unrolling can speed-up a RNN,\n            although it tends to be more memory-intensive.\n            Unrolling is only suitable for short sequences.\n    # References\n        - [Learning to forget: Continual prediction with NestedLSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n        - [Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN](https://arxiv.org/abs/1803.04831)\n    \"\"\"\n\n    def __init__(self, units,\n                 recurrent_clip_min=-1,\n                 recurrent_clip_max=-1,\n                 activation='relu',\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 recurrent_initializer=None,\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 recurrent_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 recurrent_constraint=None,\n                 bias_constraint=None,\n                 dropout=0.,\n                 recurrent_dropout=0.,\n                 implementation=2,\n                 return_sequences=False,\n                 return_state=False,\n                 go_backwards=False,\n                 stateful=False,\n                 unroll=False,\n                 **kwargs):\n        if implementation == 0:\n            warnings.warn('`implementation=0` has been deprecated, '\n                          'and now defaults to `implementation=2`.'\n                          'Please update your layer call.')\n        if K.backend() == 'theano':\n            warnings.warn(\n                'RNN dropout is no longer supported with the Theano backend '\n                'due to technical limitations. '\n                'You can either set `dropout` and `recurrent_dropout` to 0, '\n                'or use the TensorFlow backend.')\n            dropout = 0.\n            recurrent_dropout = 0.\n\n        cell = IndRNNCell(units,\n                          recurrent_clip_min=recurrent_clip_min,\n                          recurrent_clip_max=recurrent_clip_max,\n                          activation=activation,\n                          use_bias=use_bias,\n                          kernel_initializer=kernel_initializer,\n                          recurrent_initializer=recurrent_initializer,\n                          bias_initializer=bias_initializer,\n                          kernel_regularizer=kernel_regularizer,\n                          recurrent_regularizer=recurrent_regularizer,\n                          bias_regularizer=bias_regularizer,\n                          kernel_constraint=kernel_constraint,\n                          recurrent_constraint=recurrent_constraint,\n                          bias_constraint=bias_constraint,\n                          dropout=dropout,\n                          recurrent_dropout=recurrent_dropout,\n                          implementation=implementation)\n        super(IndRNN, self).__init__(cell,\n                                     return_sequences=return_sequences,\n                                     return_state=return_state,\n                                     go_backwards=go_backwards,\n                                     stateful=stateful,\n                                     unroll=unroll,\n                                     **kwargs)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n    def build(self, input_shape):\n        timesteps = input_shape[1]\n\n        if timesteps is None:\n            warnings.warn(\"Number of timesteps was not provided. If this model is being used for training purposes, \\n\"\n                          \"it is recommended to provide a finite number of timesteps when defining the input shape, \\n\"\n                          \"so as to initialize the weights of the recurrent kernel properly and avoid exploding gradients.\")\n\n        self.cell.timesteps = timesteps\n\n        super(IndRNN, self).build(input_shape)\n\n    def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n        self.cell._dropout_mask = None\n        self.cell._recurrent_masks = None\n        return super(IndRNN, self).call(inputs,\n                                        mask=mask,\n                                        training=training,\n                                        initial_state=initial_state,\n                                        constants=constants)\n\n    @property\n    def units(self):\n        return self.cell.units\n\n    @property\n    def recurrent_clip_min(self):\n        return self.cell.recurrent_clip_min\n\n    @property\n    def recurrent_clip_max(self):\n        return self.cell.recurrent_clip_max\n\n    @property\n    def activation(self):\n        return self.cell.activation\n\n    @property\n    def use_bias(self):\n        return self.cell.use_bias\n\n    @property\n    def kernel_initializer(self):\n        return self.cell.kernel_initializer\n\n    @property\n    def recurrent_initializer(self):\n        return self.cell.recurrent_initializer\n\n    @property\n    def bias_initializer(self):\n        return self.cell.bias_initializer\n\n    @property\n    def kernel_regularizer(self):\n        return self.cell.kernel_regularizer\n\n    @property\n    def recurrent_regularizer(self):\n        return self.cell.recurrent_regularizer\n\n    @property\n    def bias_regularizer(self):\n        return self.cell.bias_regularizer\n\n    @property\n    def kernel_constraint(self):\n        return self.cell.kernel_constraint\n\n    @property\n    def recurrent_constraint(self):\n        return self.cell.recurrent_constraint\n\n    @property\n    def bias_constraint(self):\n        return self.cell.bias_constraint\n\n    @property\n    def dropout(self):\n        return self.cell.dropout\n\n    @property\n    def recurrent_dropout(self):\n        return self.cell.recurrent_dropout\n\n    @property\n    def implementation(self):\n        return self.cell.implementation\n\n    def get_config(self):\n        config = {'units': self.units,\n                  'recurrent_clip_min': self.recurrent_clip_min,\n                  'recurrent_clip_max': self.recurrent_clip_max,\n                  'activation': activations.serialize(self.activation),\n                  'use_bias': self.use_bias,\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'dropout': self.dropout,\n                  'recurrent_dropout': self.recurrent_dropout,\n                  'implementation': self.implementation}\n        base_config = super(IndRNN, self).get_config()\n        del base_config['cell']\n        return dict(list(base_config.items()) + list(config.items()))\n\n    @classmethod\n    def from_config(cls, config):\n        if 'implementation' in config and config['implementation'] == 0:\n            config['implementation'] = 2\n        return cls(**config)\nregressor = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nregressor.add(GRU(units = 100, return_sequences = True, input_shape = (X_Train.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nregressor.add(GRU(units = 100, return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nregressor.add(GRU(units = 100, return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a fourth LSTM layer and some Dropout regularisation\nregressor.add(GRU(units = 50))\nregressor.add(Dropout(0.2))\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error'\n                  ,metrics=[tf.keras.metrics.RootMeanSquaredError(),\n                            tf.keras.metrics.MeanAbsoluteError(),\n                            tf.keras.metrics.MeanAbsolutePercentageError()])\nhist=regressor.fit(X_Train,\n              Y_Train, epochs = 50,\n              batch_size = 500,validation_split=0.2)\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport matplotlib\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 22}\n\nmatplotlib.rc('font', **font)\nplt.rcParams['axes.facecolor'] = 'white'\nhistory=hist\nModel_=\"IndRNN+ Optuna\"\npyplot.title(Model_+' Loss')\npyplot.plot(history.history['loss'],'r*-', label='train')\npyplot.plot(history.history['val_loss'],'b*-', label='test')\npyplot.legend()\npyplot.show()\npyplot.title(Model_+' root_mean_squared_error')\npyplot.plot(history.history['root_mean_squared_error'],'r*-', label='train')\npyplot.plot(history.history['val_root_mean_squared_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()\n\npyplot.title(Model_+' mean_absolute_error')\npyplot.plot(history.history['mean_absolute_error'],'r*-', label='train')\npyplot.plot(history.history['val_mean_absolute_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()\n\npyplot.title(Model_+' mean_absolute_percentage_error')\npyplot.plot(history.history['mean_absolute_percentage_error'],'r*-', label='train')\npyplot.plot(history.history['val_mean_absolute_percentage_error'],'b*-', label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:07.250208Z","iopub.execute_input":"2021-12-27T16:06:07.250722Z","iopub.status.idle":"2021-12-27T16:06:27.013249Z","shell.execute_reply.started":"2021-12-27T16:06:07.250686Z","shell.execute_reply":"2021-12-27T16:06:27.012452Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 30 # number of epocs per trial\nfrom typing import Any\nfrom typing import Dict\nfrom typing import Optional\nimport warnings\n\nimport optuna\n\n\nwith optuna._imports.try_import() as _imports:\n    from tensorflow.keras.callbacks import Callback\n\nif not _imports.is_successful():\n    Callback = object  # NOQA\nimport optuna\nstudy_name = 'xgbr_'  # Unique identifier of the study.\nstudy = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n                            study_name=study_name, \n                            storage='sqlite:///xgbr_4.db')\n# model creation\nimport tensorflow as tf\nimport keras \n\ndef create_lstm_model(trial):\n    train=x_train_np\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4\n    lstm_units = np.zeros(lstm_layers, dtype=np.int)\n    lstm_units[0] = trial.suggest_int(\"lstm_units_L1\", 768, 1536)\n    lstm = tf.keras.layers.Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm_units[i+1] = trial.suggest_int(\"lstm_units_L{}\".format(i+2), lstm_units[i]//2, lstm_units[i])\n        lstm = tf.keras.layers.Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    dropout_rate = trial.suggest_float(\"lstm_dropout\", 0.0, 0.3)\n    lstm = Dropout(dropout_rate)(lstm)\n    dense_units = lstm_units[-1]\n    # try different activations\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"selu\", \"elu\", \"swish\"])\n    lstm = Dense(dense_units, activation=activation)(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    metrics = [\"mae\"]\n    model.compile(optimizer=\"adam\", loss=\"mae\", metrics=metrics)\n    \n    return model\n\nclass TFKerasPruningCallback(Callback):\n    \"\"\"tf.keras callback to prune unpromising trials.\n\n    This callback is intend to be compatible for TensorFlow v1 and v2,\n    but only tested with TensorFlow v2.\n\n    See `the example <https://github.com/optuna/optuna-examples/blob/main/\n    tfkeras/tfkeras_integration.py>`__\n    if you want to add a pruning callback which observes the validation accuracy.\n\n    Args:\n        trial:\n            A :class:`~optuna.trial.Trial` corresponding to the current evaluation of the\n            objective function.\n        monitor:\n            An evaluation metric for pruning, e.g., ``val_loss`` or ``val_acc``.\n    \"\"\"\n\n    def __init__(self, trial: optuna.trial.Trial, monitor: str) -> None:\n\n        super().__init__()\n\n        _imports.check()\n\n        self._trial = trial\n        self._monitor = monitor\n\n    def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -> None:\n\n        logs = logs or {}\n        current_score = logs.get(self._monitor)\n\n        if current_score is None:\n            message = (\n                \"The metric '{}' is not in the evaluation logs for pruning. \"\n                \"Please make sure you set the correct metric name.\".format(self._monitor)\n            )\n            warnings.warn(message)\n            return\n\n        # Report current score and epoch to Optuna's trial.\n        self._trial.report(float(current_score), step=epoch)\n\n        # Prune trial if needed\n        if self._trial.should_prune():\n            message = \"Trial was pruned at epoch {}.\".format(epoch)\n            raise optuna.TrialPruned(message)\nimport enum\n\n\nclass TrialState(enum.Enum):\n    \"\"\"State of a :class:`~optuna.trial.Trial`.\n\n    Attributes:\n        RUNNING:\n            The :class:`~optuna.trial.Trial` is running.\n        WAITING:\n            The :class:`~optuna.trial.Trial` is waiting and unfinished.\n        COMPLETE:\n            The :class:`~optuna.trial.Trial` has been finished without any error.\n        PRUNED:\n            The :class:`~optuna.trial.Trial` has been pruned with\n            :class:`~optuna.exceptions.TrialPruned`.\n        FAIL:\n            The :class:`~optuna.trial.Trial` has failed due to an uncaught error.\n    \"\"\"\n\n    RUNNING = 0\n    COMPLETE = 1\n    PRUNED = 2\n    FAIL = 3\n    WAITING = 4\n\n    def __repr__(self) -> str:\n\n        return str(self)\n\n    def is_finished(self) -> bool:\n\n        return self != TrialState.RUNNING and self != TrialState.WAITING\ndef objective(trial):\n    train=x_train_np\n    \n    # Clear clutter from previous session graphs.\n    keras.backend.clear_session()\n    \n    BATCH_SIZE=512\n        # Generate our trial model.\n    model = create_lstm_model(trial)\n\n        # learning rate scheduler\n    scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 400*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n    lr = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)\n    model.fit(\n            x_train_np,\n            y_train,\n            batch_size=512,\n            callbacks=[TFKerasPruningCallback(trial, \"val_loss\")],\n            epochs=2,\n            validation_data=(x_val_np, y_val),\n            verbose=1,)\n\n        # Evaluate the model accuracy on the validation set.\n    score = model.evaluate(x_val_np, y_val, verbose=0)\n    return score[1]\nimport optuna\nstudy = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective, n_trials=100)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test= train_test_split(X_Train, Y_Train, test_size = 0.2, random_state = 0)\nX_train = np.reshape(X_train, newshape=(X_train.shape[0], X_Train.shape[1]))\nX_test = np.reshape(X_test, newshape=(X_test.shape[0], X_test.shape[1]))","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:27.014868Z","iopub.execute_input":"2021-12-27T16:06:27.015471Z","iopub.status.idle":"2021-12-27T16:06:27.065980Z","shell.execute_reply.started":"2021-12-27T16:06:27.015421Z","shell.execute_reply":"2021-12-27T16:06:27.065326Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import LinearSVR, SVR\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\ndef MAPE(Y_actual,Y_Predicted):\n    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n    return mape\nsvm_reg = SVR()\nsvm_reg.fit(X_train, y_train)\n\n# Evaluate on training set\ny_pred = svm_reg.predict(X_train)\nmse = mean_squared_error(y_train, y_pred)\nrmse = np.sqrt(mse)\nprint(\"mse:\",mse)\nprint(\"rmse:\",rmse)\nprint(\"MAPE:\",MAPE(y_train, y_pred))\nprint(\"MAE:\",mean_absolute_error(y_train, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:27.067021Z","iopub.execute_input":"2021-12-27T16:06:27.067251Z","iopub.status.idle":"2021-12-27T16:06:27.165904Z","shell.execute_reply.started":"2021-12-27T16:06:27.067219Z","shell.execute_reply":"2021-12-27T16:06:27.165078Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nsvm_reg = RandomForestRegressor(n_estimators=100)\nsvm_reg.fit(X_train, y_train)\n\n# Evaluate on training set\ny_pred = svm_reg.predict(X_train)\nmse = mean_squared_error(y_train, y_pred)\nrmse = np.sqrt(mse)\nprint(\"mse:\",mse)\nprint(\"rmse:\",rmse)\nprint(\"MAPE:\",MAPE(y_train, y_pred))\nprint(\"MAE:\",mean_absolute_error(y_train, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:27.167181Z","iopub.execute_input":"2021-12-27T16:06:27.168708Z","iopub.status.idle":"2021-12-27T16:06:28.434898Z","shell.execute_reply.started":"2021-12-27T16:06:27.168668Z","shell.execute_reply":"2021-12-27T16:06:28.434242Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nsvm_reg = GradientBoostingRegressor()\nsvm_reg.fit(X_train, y_train)\n\n# Evaluate on training set\ny_pred = svm_reg.predict(X_train)\nmse = mean_squared_error(y_train, y_pred)\nrmse = np.sqrt(mse)\nprint(\"mse:\",mse)\nprint(\"rmse:\",rmse)\nprint(\"MAPE:\",MAPE(y_train, y_pred))\nprint(\"MAE:\",mean_absolute_error(y_train, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:28.436639Z","iopub.execute_input":"2021-12-27T16:06:28.437102Z","iopub.status.idle":"2021-12-27T16:06:29.139056Z","shell.execute_reply.started":"2021-12-27T16:06:28.437063Z","shell.execute_reply":"2021-12-27T16:06:29.137864Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Shape should be Number of [Datapoints , Steps , 1 )\n# we convert into 3-d Vector or #rd Dimesnsion\nX_Train = np.reshape(X_Train, newshape=(X_Train.shape[0], X_Train.shape[1], 1))\nX_Train.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-27T16:06:29.140154Z","iopub.execute_input":"2021-12-27T16:06:29.140624Z","iopub.status.idle":"2021-12-27T16:06:29.147511Z","shell.execute_reply.started":"2021-12-27T16:06:29.140585Z","shell.execute_reply":"2021-12-27T16:06:29.146535Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}